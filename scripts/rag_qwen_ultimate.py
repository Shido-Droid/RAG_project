# scripts/rag_qwen_ultimate.py
"""
Ultimate RAG v2 (improved)
- Qwen ã«ã‚ˆã‚‹æ¤œç´¢æ„å›³åˆ¤å®š + æ„å›³ã«å¿œã˜ãŸã‚¯ã‚¨ãƒªç”Ÿæˆ
- wide -> refine ã® ddgs æ¤œç´¢
- URL ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° / å„ªå…ˆåº¦
- trafilatura + readability + bs4 ã®å …ç‰¢ãªæŠ½å‡º
- ã‚µãƒãƒªã¯ max_tokens=160ã€å¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§çŸ­ã„è‡ªå‹•è¦ç´„
- WEB_DOCS_TO_SUMMARIZE ã‚’åˆ¶é™ã—ã¦ä¸¦åˆ—/å¾…ã¡æ™‚é–“ã‚’çŸ­ç¸®
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ã€verbose ãƒ­ã‚°ã‚ã‚Š
"""
import os
import sys
import time
import json
import re
import requests
import numpy as np
from typing import List, Dict, Tuple, Any, Optional
from sentence_transformers import SentenceTransformer
import chromadb
from tqdm import tqdm

# ddgs import with fallback
try:
    from ddgs import DDGS  # preferred
except Exception:
    try:
        from duckduckgo_search import DDGS  # older package
    except Exception:
        DDGS = None

# optional libs
try:
    import trafilatura
except Exception:
    trafilatura = None

from bs4 import BeautifulSoup
try:
    from readability import Document as ReadabilityDocument
    _HAS_READABILITY = True
except Exception:
    ReadabilityDocument = None
    _HAS_READABILITY = False

# -----------------------
# Config
# -----------------------
from enum import Enum

class AnswerMode(Enum):
    NO_CONTEXT = "no_context"
    FAST_FACT = "fast_fact"
    CONTEXT_QA = "context_qa"

LMSTUDIO_URL = os.environ.get("LMSTUDIO_URL", "http://10.23.130.252:1234/v1/chat/completions")
QWEN_MODEL = os.environ.get("QWEN_MODEL", "qwen2.5-7b-instruct")
EMBED_MODEL_NAME = "intfloat/multilingual-e5-small"
CHROMA_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "chroma_db")
TOKENS_LIMIT = 2000
CHARS_LIMIT = TOKENS_LIMIT * 3
DDGS_MAX_PER_QUERY = 8
DDGS_USE_NEWS = True
NUM_SEARCH_QUERIES = 4           # reduced
WEB_DOCS_TO_SUMMARIZE = 2        # reduced to speed up
VERBOSE = True
REQUESTS_TIMEOUT = 8            # HTTP timeout
LM_TIMEOUT = int(os.environ.get("LM_TIMEOUT", "60"))   # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ LM ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰ â€” æœ€çµ‚ãƒ‘ã‚¤ãƒ—ç”¨ã¯é•·ã‚
LM_SHORT_TIMEOUT = int(os.environ.get("LM_SHORT_TIMEOUT", "12"))  # ã‚¯ã‚¨ãƒªç”Ÿæˆãªã©çŸ­ã„æ“ä½œç”¨
LM_RETRIES = int(os.environ.get("LM_RETRIES", "1"))   # ãƒªãƒˆãƒ©ã‚¤ 1 å›ï¼ˆåˆè¨ˆ2å›ï¼‰
PRIORITY_DOMAINS = [
    "tabelog.com",
    "retty.me",
    "gnavi.co.jp",
    "hotpepper.jp",
]

BOOST_KEYWORDS = [
    "å–¶æ¥­æ™‚é–“",
    "ãƒ©ãƒ³ãƒ",
    "å£ã‚³ãƒŸ",
    "è©•ä¾¡",
    "ä½æ‰€",
    "é›»è©±",
]
USER_AGENT = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"

# -----------------------
# Init models (may be slow)
# -----------------------
embed_model = SentenceTransformer(EMBED_MODEL_NAME)
client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = client.get_or_create_collection("rag_docs_e5")

# -----------------------
# Utils
# -----------------------
def log(*args, **kwargs):
    if VERBOSE:
        print(*args, **kwargs)

def safe_json_load(s: str):
    try:
        return json.loads(s)
    except Exception:
        return None
    
# -----------------------
# FAST PATH utilities
# -----------------------
def try_fast_path(question: str) -> str | None:
    # --- æ­£è¦åŒ– ---
    q = question.strip()

    # å…¨è§’ â†’ åŠè§’
    trans = str.maketrans({
        "ï¼":"0","ï¼‘":"1","ï¼’":"2","ï¼“":"3","ï¼”":"4",
        "ï¼•":"5","ï¼–":"6","ï¼—":"7","ï¼˜":"8","ï¼™":"9",
        "ï¼‹":"+","ï¼":"-","ï¼Š":"*","Ã—":"*","Ã·":"/",
        "ï¼ˆ":"(","ï¼‰":")"
    })
    q = q.translate(trans)

    # æ—¥æœ¬èªåŠ©è©ãƒ»ç–‘å•ç¬¦ãªã©é™¤å»
    q = re.sub(r"[=ã¯ï¼Ÿ\?ã‚’]", "", q)
    q = q.replace(" ", "").replace("ã€€", "")

    # --- å››å‰‡æ¼”ç®— ---
    if re.fullmatch(r"[0-9+\-*/().]+", q):
        try:
            return str(eval(q, {"__builtins__": {}}, {}))
        except Exception:
            return None

    # --- ç¾åœ¨æ™‚åˆ» ---
    if any(k in question for k in ["ç¾åœ¨ã®æ™‚åˆ»", "ä»Šä½•æ™‚", "ä»Šã®æ™‚é–“", "ä»Šã®æ™‚åˆ»", "ç¾åœ¨æ™‚åˆ»", "ä½•æ™‚ã§ã™"]):
        import datetime
        now = datetime.datetime.now(
            datetime.timezone(datetime.timedelta(hours=9))
        )
        return f"ç¾åœ¨ã®æ—¥æœ¬æ™‚åˆ»ã¯ {now.strftime('%Hæ™‚%Måˆ†')} ã§ã™ã€‚"

    # --- è¶…å¸¸è­˜ ---
    COMMON = {
        "æ—¥æœ¬ã®é¦–éƒ½": "æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚",
        "1æ—¥ã¯ä½•æ™‚é–“": "1æ—¥ã¯24æ™‚é–“ã§ã™ã€‚",
        "1å¹´ã¯ä½•æ—¥": "é€šå¸¸ã®å¹´ã¯365æ—¥ã€ã†ã‚‹ã†å¹´ã¯366æ—¥ã§ã™ã€‚",
    }
    for k, v in COMMON.items():
        if k in q:
            return v

    return None


# -----------------------
# LMStudio wrapper
# -----------------------

# ---------- lmstudio_chat ã®å·®ã—æ›¿ãˆï¼ˆç½®ãæ›ãˆï¼‰ ----------
def lmstudio_chat(
    messages: List[Dict],
    max_tokens: int = 256,
    temperature: float = 0.2,
    timeout: int = LM_TIMEOUT,
    retries: int = LM_RETRIES
) -> Dict:
    payload = {
        "model": QWEN_MODEL,   # â† å›ºå®šã‚„ã‚ã‚‹
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False
    }
    headers = {"Content-Type": "application/json"}

    last_exc = None
    for attempt in range(retries + 1):
        try:
            r = requests.post(
                LMSTUDIO_URL,
                headers=headers,
                data=json.dumps(payload),
                timeout=timeout
            )
            r.raise_for_status()
            time.sleep(0.3)  # â˜… qwen2.5 å®‰å®šåŒ–ï¼ˆé‡è¦ï¼‰
            return r.json()
        except requests.exceptions.Timeout as e:
            last_exc = e
            if attempt < retries:
                log(f"[lmstudio_chat] Timeout retry {attempt+1}/{retries}")
                continue
            raise RuntimeError("LM timeout") from e
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"[lmstudio_chat] HTTP error: {e}") from e

    raise RuntimeError(f"[lmstudio_chat] failed: {last_exc}")


    # should not reach here
    raise RuntimeError(f"[lmstudio_chat] Unknown failure: {last_exc}")
# ------------------------------------------------------------------

# -----------------------
# 0) Intent detection helper
# -----------------------
def detect_search_intent(question: str, history: List[Dict] = []) -> str:
    """
    Ask Qwen to classify intent: informational / local_search / news / other
    If LM fails, fallback simple heuristic:
      - contains words like 'ã©ã“', 'è¿‘ã', 'ãƒ©ãƒ³ãƒ', 'åº—' -> local_search
      - contains 'ã„ã¤', 'ãªãœ', 'ã©ã†ã‚„ã£ã¦' -> informational
      - contains 'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'ç™ºè¡¨' -> news
      - else informational
    """
    # 1. Fast heuristics (Prioritize document/local keywords)
    qlow = question.lower()
    doc_tokens = ["ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ", "ã“ã®æ–‡æ›¸", "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ãƒ•ã‚¡ã‚¤ãƒ«", "è³‡æ–™", "pdf", "è¦ç´„", "æŠ½å‡º", "ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "ç« "]
    for tok in doc_tokens:
        if tok in qlow:
            log(f"[Intent] Heuristic match: '{tok}' -> document_qa")
            return "document_qa"

    system = (
    "Classify intent: informational / spec / factual / local_search / news / weather / document_qa / other"
    )

    history_text = ""
    if history:
        history_text = "ä¼šè©±å±¥æ­´:\n" + "\n".join([f"- {h['role']}: {h['content']}" for h in history]) + "\n\n"

    user = f"{history_text}ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼ˆæ—¥æœ¬èªï¼‰: {question}\n\nReturn one of: informational, local_search, news, weather, document_qa, other"
    try:
        resp = lmstudio_chat(
            [{"role":"system","content":system},
             {"role":"user","content":user}],
            max_tokens=32,
            temperature=0.0,
            timeout=LM_SHORT_TIMEOUT   # â† è¿½åŠ 
        )

        text = resp['choices'][0]['message']['content'].strip().lower()
        for t in ["informational","local_search","news","weather","document_qa","other"]:
            if t in text:
                return t
    except Exception as e:
        log("[Intent] LM failed:", e)
    # fallback heuristics
    local_tokens = ["è¿‘ã", "ãƒ©ãƒ³ãƒ", "åº—", "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³", "å–¶æ¥­æ™‚é–“", "ãŠã„ã—ã„", "äºˆç´„"]
    news_tokens = ["ãƒ‹ãƒ¥ãƒ¼ã‚¹", "ç™ºè¡¨", "é€Ÿå ±", "æ˜¨æ—¥", "ä»Šæ—¥"]
    info_tokens = ["ãªãœ", "ã©ã†ã‚„ã£ã¦", "ã„ã¤", "ã¨ã¯", "æ•™ãˆã¦", "æ¨™é«˜", "å®šç¾©", "æ„å‘³"]
    spec_tokens = ["ãƒãƒ¼ã‚¸ãƒ§ãƒ³", "ä»•æ§˜", "å¯¾å¿œ", "api", "model", "release"]
    weather_tokens = ["å¤©æ°—", "äºˆå ±", "æ°—æ¸©", "é›¨", "æ™´ã‚Œ", "å°é¢¨", "æ°—è±¡"]

    if any(tok in qlow for tok in weather_tokens):
        return "weather"
    if any(tok in qlow for tok in local_tokens):
        return "local_search"
    if any(tok in qlow for tok in news_tokens):
        return "news"
    if any(tok in qlow for tok in info_tokens):
        return "informational"
    if any(tok in qlow for tok in spec_tokens):
        return "spec"
    return "informational"

# -----------------------
# 1) Query generation (intent-aware)
# -----------------------
def qwen_generate_search_queries(question: str, intent: str, history: List[Dict] = [], n: int = NUM_SEARCH_QUERIES) -> List[str]:
    log("[Search Intent]", intent)
    # build a system prompt tailored by intent
    if intent == "local_search":
        sys_prompt = ("You are a search-query generator for local business searches (Japanese). Produce short, location-aware queries likely to hit local review/restaurant pages.")
        extra_instruction = "- Prefer terms like 'ãƒ©ãƒ³ãƒ', 'å–¶æ¥­æ™‚é–“', 'å£ã‚³ãƒŸ', 'é£Ÿã¹ãƒ­ã‚°', 'ä½æ‰€' etc."
    elif intent == "news":
        sys_prompt = ("You are a search-query generator for news-related searches (Japanese). Produce concise queries that would match news articles and official sources.")
        extra_instruction = "- Prefer terms like 'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'é€Ÿå ±', 'ç™ºè¡¨', 'åŸå› ', 'å½±éŸ¿'."
    elif intent == "weather":
        sys_prompt = ("You are a search-query generator for weather forecasts (Japanese). Produce queries to get accurate weather info.")
        extra_instruction = "- Prefer terms like 'å¤©æ°—', '1æ™‚é–“ã”ã¨', 'é€±é–“äºˆå ±', 'æ°—è±¡åº'."
    elif intent == "informational":
        sys_prompt = (
        "You are a search-query generator for factual informational search (Japanese). "
        "If the query is an acronym or ambiguous, add context (e.g. 'AI', 'IT', 'æ„å‘³') or expand it. "
        "DO NOT add restaurant, food, travel, or local business related terms unless explicitly asked."
        )
        extra_instruction = "- Use factual terms. Expand acronyms if ambiguous."
    elif intent == "recommendation":
        sys_prompt = (
        "You are a search-query generator for movie recommendations (Japanese). "
        "Generate queries about currently showing movies, rankings, and reviews. "
        "DO NOT include restaurants or food-related terms."
        )
        extra_instruction = "- Prefer terms like 'å…¬é–‹ä¸­ æ˜ ç”»', 'æ˜ ç”» ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'è©•ä¾¡'."
    else:
        # intent == "other" ç”¨ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰
        sys_prompt = (
            "You are a search-query generator for general informational search (Japanese). "
            "Avoid restaurant, food, travel, and local business terms."
        )
        extra_instruction = "- Use neutral factual keywords only."

    history_text = ""
    if history:
        history_text = "ä¼šè©±å±¥æ­´:\n" + "\n".join([f"- {h['role']}: {h['content']}" for h in history]) + "\n\n"

    user = (
        f"{history_text}ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}\n\n"
        f"å‡ºåŠ›ãƒ«ãƒ¼ãƒ«:\n- {extra_instruction}\n- å‡ºåŠ›ã¯JSONé…åˆ—ï¼ˆæ—¥æœ¬èªã®æ–‡å­—åˆ—é…åˆ—ï¼‰ã§1è¡Œã§è¿”ã—ã¦ãã ã•ã„ã€‚\n"
        f"å‡ºåŠ›ãƒ«ãƒ¼ãƒ«:\n- {extra_instruction}\n"
        f"- Generate {n} different queries.\n"
        f"- å‡ºåŠ›ã¯JSONé…åˆ—ï¼ˆæ—¥æœ¬èªã®æ–‡å­—åˆ—é…åˆ—ï¼‰ã§1è¡Œã§è¿”ã—ã¦ãã ã•ã„ã€‚\n"
        f"- ä¾‹: [\"å¯Œå£«å±± æ¨™é«˜\", \"å¯Œå£«å±± é«˜ã• å…¬å¼\"]"
    )

    messages = [{"role":"system","content":sys_prompt},{"role":"user","content":user}]
    try:
        resp = lmstudio_chat(messages=messages, max_tokens=160, temperature=0.0, timeout=LM_SHORT_TIMEOUT)
        text = resp['choices'][0]['message']['content']
        parsed = safe_json_load(text)
        if isinstance(parsed, list) and parsed:
            qs = [q.strip() for q in parsed if isinstance(q, str) and q.strip()]
            return qs[:n]
        # fallback: try line extraction
        lines = [l.strip(" -â€¢\"'") for l in text.splitlines() if l.strip()]
        qs = []
        for ln in lines:
            ln2 = re.sub(r'^[0-9]+[).:\-\s]*', '', ln)
            if ln2:
                qs.append(ln2)
            if len(qs) >= n:
                break
        if qs:
            return qs[:n]
    except Exception as e:
        log("[Qwen] query-gen error (LM):", e)

    # LM failed => fallback heuristics depending on intent
    base = question.strip()
    if intent == "local_search":
        variants = [f"{base} ãƒ©ãƒ³ãƒ", f"{base} å–¶æ¥­æ™‚é–“", f"{base} å£ã‚³ãƒŸ", f"{base} é£Ÿã¹ãƒ­ã‚°"]
    elif intent == "news":
        variants = [f"{base} ãƒ‹ãƒ¥ãƒ¼ã‚¹", f"{base} é€Ÿå ±", f"{base} ç™ºè¡¨"]
    elif intent == "weather":
        variants = [f"{base} å¤©æ°—", f"{base} äºˆå ±", f"{base} æ°—è±¡åº"]
    else:
        variants = [base, base + " ã¨ã¯", base + " æ„å‘³", base + " ãƒ‡ãƒ¼ã‚¿"]
    # ensure length n
    out = []
    for v in variants:
        if v not in out:
            out.append(v)
        if len(out) >= n:
            break
    while len(out) < n:
        out.append(base)
    return out[:n]

# -----------------------
# 2) ddgs search (wide -> refine)
# -----------------------
def ddgs_search_many(queries: List[str], per_query: int = DDGS_MAX_PER_QUERY) -> List[Dict]:
    results = []
    if DDGS is None:
        log("[DDGS] ddgs/duckduckgo not available.")
        return results

    try:
        ddgs: Any = DDGS()
        for i, q in enumerate(queries):
            if i > 0:
                time.sleep(1.0)  # é€£ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã‚ˆã‚‹gzipã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚å¾…æ©Ÿ
            log("[DDGS] Searching:", q)
            try:
                for r in ddgs.text(q, region="jp-jp", safesearch="off", timelimit=None, max_results=per_query):
                    if r.get("href"):
                        results.append({"title": r.get("title",""), "body": r.get("body",""), "href": r.get("href",""), "query": q})
                if DDGS_USE_NEWS:
                    try:
                        for r in ddgs.news(q, region="jp-jp", max_results=4):
                            if r.get("href"):
                                results.append({"title": r.get("title",""), "body": r.get("body",""), "href": r.get("href",""), "query": q})
                    except Exception:
                        pass
            except Exception as e:
                log("[DDGS] search error:", q, e)
    except Exception as e:
        log("[DDGS] init/search error:", e)

    # dedupe
    uniq = {}
    for r in results:
        href = r.get("href") or ""
        key = href or (r.get("title","")+r.get("body",""))
        if key not in uniq:
            uniq[key] = r
    out = list(uniq.values())
    log(f"[DDGS] Found {len(out)} unique hits")
    return out

def refine_queries_from_hits(
    hits: List[Dict],
    n_extra: int = 2,
    *,
    intent: str | None = None,
) -> List[str]:
    """
    Generate additional search queries from top search hits.
    - intent ãŒ local_search / weather ç³»ã®å ´åˆã¯ refine ã—ãªã„
    """

    # ---- intent ã‚¬ãƒ¼ãƒ‰ï¼ˆæœ€é‡è¦ï¼‰----
    if intent in ("local_search", "weather", "time", "calculator"):
        return []

    if not hits:
        return []

    top_text = "\n".join(
        [
            f"{i+1}. {h.get('title','')} - {h.get('body','')}"
            for i, h in enumerate(hits[:8])
        ]
    )

    prompt = (
        "ä»¥ä¸‹ã¯æ¤œç´¢ä¸Šä½ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã™ã€‚"
        "ã“ã‚Œã‚’å…ƒã«ã•ã‚‰ã«æ˜ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¯ã‚¨ãƒªã‚’"
        f"æ—¥æœ¬èªã§{n_extra}å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆçŸ­ãï¼‰ã€‚\n\n{top_text}"
    )

    try:
        resp = lmstudio_chat(
            [
                {"role": "system", "content": "You are a search optimizer."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=120,
            temperature=0.0,
            timeout=12,
        )

        text = resp["choices"][0]["message"]["content"]

        lines = [l.strip(" -â€¢\"'") for l in text.splitlines() if l.strip()]
        out: List[str] = []

        for ln in lines:
            ln2 = re.sub(r"^[0-9]+[).:\-\s]*", "", ln)
            if ln2:
                out.append(ln2)
            if len(out) >= n_extra:
                break

        return out

    except Exception as e:
        log("[Qwen] refine-queries error:", e)
        return []

# -----------------------
# 3) fetching & extraction
# -----------------------
def fetch_html(url: str) -> str:
    if not url:
        return ""
    headers = {"User-Agent": USER_AGENT}
    try:
        r = requests.get(url, headers=headers, timeout=REQUESTS_TIMEOUT)
        if r.status_code == 200 and r.content:
            # â˜… charset ã‚’å¼·åˆ¶ UTF-8
            r.encoding = r.apparent_encoding or "utf-8"
            return r.text
    except Exception as e:
        log("[fetch_html] error:", url, e)
    return ""


BLACKLIST_DOMAINS = [
    "doubleclick.net",
    "facebook.com",
    "twitter.com",
    "x.com",
    "youtube.com",
    "bing.com",
    "tiktok.com",
    "instagram.com",
]

WHITELIST_DOMAINS = [
    "ai.google.dev",
    "developers.google.com",
    "cloud.google.com",
    "gemini.google.com",
    "openai.com",
    "docs.openai.com",
]

from urllib.parse import urlparse

def extract_text(url: str, html: Optional[str] = None) -> str:
    """
    Robust extraction:
    - domain blacklist(fast skip) 
    - BUT keep offical Whitelist
    """
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    
    # whitelist å„ªå…ˆ
    if not any(w in domain for w in WHITELIST_DOMAINS):
        if any(b in domain for b in BLACKLIST_DOMAINS):
            log(f"[extract_text] skipped by blacklist: {url}")
            return ""
        if "xn--" in domain and not domain.endswith(".jp"):
            log(f"[extract_text] skipped suspicious punycode: {url}")
            return ""


    if html is None:
        html = fetch_html(url)

    if not html or len(html) < 200:
        log(f"[extract_text] empty HTML for {url}")
        return ""

    # Sanitize HTML to prevent lxml errors with null bytes/control chars
    html = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', html)

    # Trafilatura
    if trafilatura is not None:
        try:
            txt = trafilatura.extract(html, include_comments=False, favor_precision=True)
            if txt and len(txt.strip()) > 220:
                return txt.strip()
        except Exception:
            pass

    # Readability
    if _HAS_READABILITY and ReadabilityDocument:
        try:
            doc = ReadabilityDocument(html)
            summary = doc.summary()
            soup = BeautifulSoup(summary, "html.parser")
            text = soup.get_text("\n", strip=True)
            if text and len(text) > 140:
                return text
        except Exception:
            pass

    # BeautifulSoup heavy cleaning
    try:
        soup = BeautifulSoup(html, "html.parser")
        for bad in soup(["script","style","noscript","header","footer","nav","aside","form","iframe","svg"]):
            bad.decompose()
        body = soup.body or soup
        raw = body.get_text("\n", strip=True)
        lines = []
        for ln in raw.splitlines():
            ln = ln.strip()
            if not ln:
                continue
            if len(ln) < 30:
                continue
            if any(x in ln for x in ["åˆ©ç”¨è¦ç´„","Cookie","Privacy","ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼"]):
                continue
            lines.append(ln)
        if lines:
            text = "\n\n".join(lines)
            return text[:30000]
    except Exception:
        pass

    # Final minimal fallback: title + first 15 meaningful lines
    try:
        soup = BeautifulSoup(html, "html.parser")
        title = soup.title.get_text().strip() if soup.title else ""
        alltxt = soup.get_text("\n", strip=True)
        lines = [l.strip() for l in alltxt.splitlines() if l.strip() and len(l.strip()) >= 30]
        body = "\n".join(lines[:15])
        if title or body:
            return f"{title}\n{body}"
    except Exception:
        pass

    return ""

# -----------------------
# 4) scoring (kept but less aggressive)
# -----------------------
def score_text_for_restaurant(text: str, title: str = "", url: str = "") -> float:
    score = 0.0
    lower = (title + "\n" + (text or "")).lower()
    for dom in PRIORITY_DOMAINS:
        if dom in (url or "").lower() or dom in lower:
            score += 2.5
    for k in BOOST_KEYWORDS:
        cnt = lower.count(k.lower())
        if cnt:
            score += min(2.0, 0.4 * cnt)
    if re.search(r"\d{2,4}-\d{1,4}", lower) or "ã€’" in lower:
        score += 1.0
    if len(text) > 800:
        score += 1.0
    elif len(text) > 300:
        score += 0.4
    return score

def score_text_for_spec(text: str, title: str = "", url: str = "") -> float:
    score = 0.0
    t = (title + " " + text).lower()

    # å…¬å¼ãƒ»ä¸€æ¬¡æƒ…å ±ã‚’å¼·ãè©•ä¾¡
    if any(k in url for k in ["google.com", "ai.google.dev"]):
        score += 3.0

    # specç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    spec_keywords = [
        "version", "ãƒãƒ¼ã‚¸ãƒ§ãƒ³", "release", "changelog",
        "api", "model", "ä»•æ§˜", "å¯¾å¿œ", "æ›´æ–°"
    ]
    score += sum(0.3 for k in spec_keywords if k in t)

    # æ•°å­—ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¡¨è¨˜
    if any(ch.isdigit() for ch in text):
        score += 0.5

    # æ—¥ä»˜ãŒã‚ã‚‹ã¨åŠ ç‚¹
    if any(k in t for k in ["2024", "2025", "æœˆ", "æ—¥"]):
        score += 0.5

    return score

# -----------------------
# 5) summarization & extraction (LM with small max_tokens + fast fallback)
# -----------------------


# -----------------------
# 6) Chroma search
# -----------------------
def search_chroma(query: str, n_results: int = 6) -> List[Dict]:
    try:
        q_emb = embed_model.encode([f"query: {query}"])
        res = collection.query(query_embeddings=[q_emb[0]], n_results=n_results)
        
        documents = res.get("documents")
        docs = documents[0] if documents else []
        
        metadatas = res.get("metadatas")
        metas = metadatas[0] if metadatas else []
        
        # docs/metas ãŒ None ã®å ´åˆã®ã‚¬ãƒ¼ãƒ‰ (Chromaã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã‚‹æŒ™å‹•å·®ç•°å¸å)
        if docs is None: docs = []
        if metas is None: metas = []
        
        results = []
        for d, m in zip(docs, metas):
            results.append({"text": d, "meta": m or {}})
        return results[:n_results]
    except Exception as e:
        log("[Chroma] query error:", e)
        return []

# -----------------------
# 7) context builder
# -----------------------

def collect_candidates(chroma_docs, scored_web, min_chars: int = 50):
    """
    Chroma + Web ã‚’çµ±åˆã—ã¦å€™è£œã‚’ä½œã‚‹
    - text ãŒ min_chars æœªæº€ã®ã‚‚ã®ã¯é™¤å¤–
    """
    candidates = []

    # ---- Chroma docs ----
    for item in chroma_docs:
        text = item.get("text", "").strip()
        if len(text) < min_chars:
            continue

        meta = item.get("meta", {})
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚„ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
        title = meta.get("title") or meta.get("source") or "Local Document"

        candidates.append({
            "source": "chroma",
            "text": text,
            "meta": {"title": title, "url": meta.get("source")}
        })

    # ---- Web docs ----
    for item in scored_web:
        text = (item.get("text") or "").strip()
        if len(text) < min_chars:
            continue

        candidates.append({
            "source": "web",
            "text": text,
            "meta": {
                "title": item.get("title"),
                "url": item.get("url")
            }
        })

    return candidates

    
def rerank_candidates(question, candidates, top_k=8):
    q_emb = embed_model.encode([f"query: {question}"])[0]
    ranked = []

    for c in candidates:
        if "emb" not in c:
            c["emb"] = embed_model.encode([f"passage: {c['text'][:800]}"])[0]
            
        emb = c["emb"]
        score = float(
            np.dot(q_emb, emb) / 
            (np.linalg.norm(q_emb) * np.linalg.norm(emb) + 1e-8)
        )
        ranked.append((score, c))

    ranked.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in ranked[:top_k]]
    
def dedupe_by_similarity(candidates, threshold=0.92):
    """
    embedding é¡ä¼¼åº¦ãŒé«˜ã™ãã‚‹æ–‡æ›¸ã‚’é™¤å¤–ã™ã‚‹
    - rerank_candidates å¾Œã® candidates ã‚’æƒ³å®š
    - c["emb"] ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å‰æ
    """
    deduped = []

    for c in candidates:
        keep = True
        for o in deduped:
            sim = float(
                np.dot(c["emb"], o["emb"]) /
                (np.linalg.norm(c["emb"]) * np.linalg.norm(o["emb"]) + 1e-8)
            )
            if sim >= threshold:
                keep = False
                break

        if keep:
            deduped.append(c)

    return deduped


    
def build_context_from_candidates(candidates, char_limit=CHARS_LIMIT):
    buf = []
    total = 0

    for c in candidates:
        if c["source"] == "web":
            header = f"[Web]\nTitle: {c['meta'].get('title')}\nURL: {c['meta'].get('url')}\n"
        else:
            header = f"[Document: {c['meta'].get('title')}]\n"

        body = c["text"].strip()
        chunk = header + body + "\n\n"

        if total + len(chunk) > char_limit:
            break

        buf.append(chunk)
        total += len(chunk)

    return "".join(buf)

# -----------------------
# 8) final answer pipeline
# -----------------------
# ---------- final_answer_pipeline ã® LM å‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆèª¿æ•´ï¼ˆç½®ãæ›ãˆï¼‰ ---------

def final_answer_pipeline(question: str, context: str, history: List[Dict] = [], intent: str = "informational", difficulty: str = "normal") -> str:
    """
    Final answer generation for RAG (non-silent version)
    - Extract answers explicitly stated in context
    - If partially answerable, answer only that part
    - If nothing relevant exists, say so
    """

    if intent == "weather":
        system = (
            "ã‚ãªãŸã¯å¤©æ°—äºˆå ±ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
            "ã€æ¤œç´¢ã•ã‚ŒãŸæ–‡è„ˆã€‘ã«ã‚ã‚‹æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆæ°—æ¸©ã€é™æ°´ç¢ºç‡ã€é¢¨é€Ÿãªã©ï¼‰ã‚’æ•´ç†ã—ã¦ä¼ãˆã¦ãã ã•ã„ã€‚\n"
            "å¤©å€™ï¼ˆæ™´ã‚Œãƒ»é›¨ãªã©ï¼‰ã®æ˜ç¤ºçš„ãªè¨˜è¿°ãŒãªãã¦ã‚‚ã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ãã‚Œã‚’å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
            "æ–‡è„ˆã«æ—¥ä»˜ã‚„æ™‚åˆ»ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã‚‚æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚"
        )
    else:
        base_system = (
            "ã‚ãªãŸã¯ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
            "æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
            "ä»¥ä¸‹ã®ã€æ¤œç´¢ã•ã‚ŒãŸæ–‡è„ˆã€‘ã«å«ã¾ã‚Œã¦ã„ã‚‹æƒ…å ±ã ã‘ã‚’ä½¿ã£ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n"
            "ã‚‚ã—æ–‡è„ˆã®ä¸­ã«ç­”ãˆãŒå…¨ããªã„å ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯åˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚\n"
            "å›ç­”ã§ããŸå ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯åˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ã„ã†æ–‡è¨€ã‚’çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n"
            "æ±ºã—ã¦è‡ªåˆ†ã®çŸ¥è­˜ã‚’ä½¿ã£ã¦å›ç­”ã‚’æé€ ã—ãŸã‚Šã€æ–‡è„ˆã«ãªã„æƒ…å ±ã‚’è¿½åŠ ã—ãŸã‚Šã—ãªã„ã§ãã ã•ã„ã€‚\n"
            "ã‚‚ã—æ–‡è„ˆã®ä¸­ã«ã€Œ[Section: ...]ã€ã‚„ã€Œ[Page X]ã€ã®ã‚ˆã†ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å›ç­”ã®æ–‡æœ«ã«ã€Œ(å‚ç…§: Section '...', Page X)ã€ã®ã‚ˆã†ã«ä»˜è¨˜ã—ã¦ãã ã•ã„ã€‚\n"
            "ã€é‡è¦ã€‘å›ç­”ã«å°‚é–€ç”¨èªã‚’å«ã‚ã‚‹å ´åˆã¯ã€å¿…ãšãã®ç”¨èªã‚’ `[[å°‚é–€ç”¨èª]]` ã®ã‚ˆã†ã«äºŒé‡è§’æ‹¬å¼§ã§å›²ã£ã¦ãã ã•ã„ã€‚ä¾‹: ã€Œã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯[[RAG]]ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚ã€"

        )
        
        if difficulty == "easy":
            system = base_system + "\n\nã€å›ç­”ã‚¹ã‚¿ã‚¤ãƒ«: åˆå­¦è€…å‘ã‘ã€‘\nå°‚é–€ç”¨èªã¯ãªã‚‹ã¹ãé¿ã‘ã€åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã§ã€ä¸å¯§ã«å™›ã¿ç •ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€é‡è¦ãªå›ºæœ‰åè©ã‚„ç”¨èªã¯å¿…ãš `[[ ]]` ã§å›²ã£ã¦ãã ã•ã„ã€‚"
        elif difficulty == "professional":
            system = base_system + "\n\nã€å›ç­”ã‚¹ã‚¿ã‚¤ãƒ«: å°‚é–€çš„ã€‘\nå°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã€ç°¡æ½”ã‹ã¤è«–ç†çš„ã«ã€å®Ÿå‹™çš„ãƒ»å°‚é–€çš„ãªè¦³ç‚¹ã‹ã‚‰è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªç”¨èªã¯å¿…ãš `[[ ]]` ã§å›²ã£ã¦ãã ã•ã„ã€‚"
        else:
            system = base_system

    def _try_generate(ctx):
        history_text = ""
        if history:
            history_text = "Conversation History:\n" + "\n".join([f"{h['role']}: {h['content']}" for h in history]) + "\n\n"

        user = (
            f"{history_text}ã€æ¤œç´¢ã•ã‚ŒãŸæ–‡è„ˆã€‘:\n{ctx}\n\n"
            f"ã€è³ªå•ã€‘:\n{question}\n\n"
            "ã€æŒ‡ç¤ºã€‘:\n"
            "å›ç­”ã«å«ã¾ã‚Œã‚‹é‡è¦ãªå°‚é–€ç”¨èªã€ã‚·ã‚¹ãƒ†ãƒ åã€æ©Ÿèƒ½åãªã©ã¯ã€å¿…ãš `[[ç”¨èª]]` ã®ã‚ˆã†ã«äºŒé‡è§’æ‹¬å¼§ã§å›²ã£ã¦ãã ã•ã„ã€‚\n"
            "ä¾‹: ã€Œ[[ç„¡é™å¤§ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹]]ã§ã¯[[å±¥ä¿®ç™»éŒ²]]ãŒå¯èƒ½ã§ã™ã€‚ã€"
        )
        return lmstudio_chat(
            [
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            max_tokens=512,
            temperature=0.0,
            timeout=LM_TIMEOUT,
        )

    try:
        resp = _try_generate(context)
        content = resp["choices"][0]["message"]["content"].strip()

        # Post-processing: å›ç­”ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã®ã«ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€å‰Šé™¤ã™ã‚‹
        failure_phrase = "æä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯åˆ†ã‹ã‚Šã¾ã›ã‚“"
        if failure_phrase in content and len(content) > 50:
            content = content.replace(failure_phrase, "")

        return content.strip()

    except Exception as e:
        log("[Qwen] final_answer_pipeline error:", e)
        if "400" in str(e):
            log("[Qwen] 400 Error detected. Retrying with shorter context...")
            try:
                resp = _try_generate(context[:len(context)//2])
                return resp["choices"][0]["message"]["content"].strip()
            except Exception as e2:
                log("[Qwen] Retry failed:", e2)
        return "å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"


def build_recommendation_answer(web_summaries, question):
    """
    recommendation ç”¨ï¼ˆLMã‚ã‚Šï¼‰
    - ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’çµ±åˆã—ã¦å›ç­”ç”Ÿæˆ
    - é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’å«ã‚€å ´åˆã¯è¦ç´„ã®ã¿ã‚’å«ã‚ã‚‹
    """
    context = "\n".join(
        f"- {title}: {summary}"
        for title, summary, _ in web_summaries[:3]
    )

    prompt = f"""
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{question}

ã€å‚è€ƒæƒ…å ±ã€‘
{context}

ãƒ»æ¨æ¸¬ã¯ã—ãªã„
ãƒ»ä¸æ˜ãªå ´åˆã¯ã€Œå…¬å¼ã«æ˜ç¤ºã•ã‚Œã¦ã„ãªã„ã€ã¨æ›¸ã
ãƒ»æœ€æ–°æƒ…å ±ãŒã‚ã‚Œã°æ—¥ä»˜ã‚’æ˜è¨˜ã™ã‚‹
"""

    resp = lmstudio_chat(
        [
            {"role": "system", "content": "You are a precise technical assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=512,
        temperature=0.0,
    )

    return resp["choices"][0]["message"]["content"].strip()




# =========================
# Question analysis helpers
# =========================
def decide_answer_mode(intent: str, context: str, web_list) -> AnswerMode:
    # context ãŒå®Œå…¨ã«ç©º â†’ ä½•ã‚‚ç­”ãˆã‚‰ã‚Œãªã„
    if not context or len(context.strip()) < 100:
        return AnswerMode.NO_CONTEXT

    # informational ã¯ FAST_FACT å›ºå®š
    if intent == "informational":
        return AnswerMode.FAST_FACT

    # ãã‚Œä»¥å¤–ã¯ context QAï¼ˆspec / factual / news / localï¼‰
    return AnswerMode.CONTEXT_QA


def extract_keywords_ja(question: str) -> list[str]:
    q = question.replace("ï¼Ÿ", "").replace("?", "")
    stop = {"ã¯", "ã¨", "ã®", "ãŒ", "ã‚’", "ã«", "ã§ã™", "ä½•"}

    keywords = []

    # æ„å‘³ç³»ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆ
    for w in ["é•ã„", "æ¯”è¼ƒ", "æ„å‘³", "ç†ç”±", "ç‰¹å¾´", "æ–¹æ³•", "ç¨®é¡"]:
        if w in q:
            keywords.append(w)

    # åè©ã£ã½ã„æ–‡å­—ã‚‚æ‹¾ã†ï¼ˆè¶…ç°¡æ˜“ï¼‰
    for ch in q:
        if ch not in stop and ch not in keywords:
            keywords.append(ch)

    return keywords



# =========================
# Answer builders
# =========================

def build_informational_answer(web_summaries, question):
    """
    informational ç”¨ï¼ˆLMãªã—ï¼‰
    - ã‚¿ã‚¤ãƒˆãƒ«ãŒè³ªå•èªã¨ç„¡é–¢ä¿‚ãªã‚‚ã®ã‚’é™¤å¤–
    """
    keywords = extract_keywords_ja(question)
    lines = []

    for title, summary, url in web_summaries:
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºãªã‚‰é™¤å¤–
        if not title:
            continue

        # ğŸ” è³ªå•ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1ã¤ã‚‚å«ã¾ãªã„ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
        if not any(k in title for k in keywords):
            continue

        lines.append(summary.strip())

        # æœ€å¤§2ä»¶ã¾ã§
        if len(lines) >= 2:
            break

    # ä¿é™ºï¼š1ä»¶ã‚‚æ®‹ã‚‰ãªã‹ã£ãŸå ´åˆ
    if not lines and web_summaries:
        lines.append(web_summaries[0][1].strip())

    return "\n".join(lines)

def build_spec_answer(web_summaries, question):
    """
    spec / factual ç”¨ï¼ˆLMã‚ã‚Šï¼‰
    ãƒ»è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆ
    ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ / å‹ç•ª / æ—¥ä»˜ã‚’æ˜ç¤º
    """
    context = "\n".join(
        f"- {title}: {summary}"
        for title, summary, _ in web_summaries[:3]
    )

    prompt = f"""
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{question}

ã€å‚è€ƒæƒ…å ±ã€‘
{context}

ãƒ»æ¨æ¸¬ã¯ã—ãªã„
ãƒ»ä¸æ˜ãªå ´åˆã¯ã€Œå…¬å¼ã«æ˜ç¤ºã•ã‚Œã¦ã„ãªã„ã€ã¨æ›¸ã
ãƒ»æœ€æ–°æƒ…å ±ãŒã‚ã‚Œã°æ—¥ä»˜ã‚’æ˜è¨˜ã™ã‚‹
"""

    resp = lmstudio_chat(
        [
            {"role": "system", "content": "You are a precise technical assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=512,
        temperature=0.0,
    )

    return resp["choices"][0]["message"]["content"].strip()


# -----------------------
# Main flow
# -----------------------
def process_question(question: str, history: List[Dict] = [], difficulty: str = "normal") -> dict:
    # ===== FAST PATH =====
    fast = try_fast_path(question)
    if fast is not None:
        return {"answer": fast, "sources": []}
    start_time = time.time()

    # =====================
    # STEP 0: intentï¼ˆ1å›ã ã‘ï¼‰
    # =====================
    intent = detect_search_intent(question, history)
    log(f"[Intent] {intent}")

    # =====================
    # STEP 1: Chroma
    # =====================
    log("=== STEP 1: Chroma æ¤œç´¢ ===")
    chroma_docs = search_chroma(question, n_results=10)
    for i, d in enumerate(chroma_docs, 1):
        log(f"[Chroma #{i}] {str(d.get('text'))[:100].replace(chr(10),' ')}... (Meta: {d.get('meta')})")

    # =====================
    # STEP 2: Search queries
    # =====================
    if intent == "document_qa":
        log("=== STEP 2 & 3: Web search skipped (document_qa) ===")
        queries = []
        hits = []
    else:
        log("=== STEP 2: æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ ===")
        queries = qwen_generate_search_queries(question, intent, history, n=NUM_SEARCH_QUERIES)
        log("Generated queries:", queries)

        # =====================
        # STEP 3: ddgs wide
        # =====================
        log("=== STEP 3: ddgs wide search ===")
        hits = ddgs_search_many(queries, per_query=DDGS_MAX_PER_QUERY)

        # intent ã«ã‚ˆã‚‹ä»¶æ•°åˆ¶å¾¡
        if intent == "informational":
            hits = hits[:5]
        elif intent in ("local_search", "news", "recommendation", "weather"):
            hits = hits[:10]

    # =====================""" 
    # STEP 4: refineï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
    # =====================
    log("=== STEP 4: refine search ===")
    if intent in ("local_search", "news", "recommendation"):
        extra = refine_queries_from_hits(hits, n_extra=2, intent=intent)
        if extra:
            log("Refined queries:", extra)
            more_hits = ddgs_search_many(extra, per_query=6)

            seen = {h.get("href") for h in hits if h.get("href")}
            for h in more_hits:
                if h.get("href") and h["href"] not in seen:
                    hits.append(h)

    # =====================
    # STEP 5: unique + fetch + score
    # =====================
    unique_hits = []
    seen = set()
    for h in hits:
        key = h.get("href") or (h.get("title","") + h.get("body",""))
        if not key or key in seen:
            continue
        seen.add(key)
        unique_hits.append(h)

    log(f"[Total unique hits] {len(unique_hits)}")

    scored = []
    for h in unique_hits:
        url = h.get("href","")
        title = h.get("title","")
        text = extract_text(url)

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¤œç´¢ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’åˆ©ç”¨
        if not text or len(text) < 50:
            snippet = h.get("body", "")
            if snippet and len(snippet) > 30:
                text = f"{snippet}\n(Note: Full content fetch failed, using search snippet.)"

        if not text:
            continue

        if intent in ("spec", "factual", "informational", "weather"):
            score = score_text_for_spec(text, title=title, url=url)
        else:
            score = score_text_for_restaurant(text, title=title, url=url)

        scored.append({
            "title": title,
            "url": url,
            "text": text,
            "score": score
        })

    scored.sort(key=lambda x: x["score"], reverse=True)
    log(f"[Web] scored items: {len(scored)}")


    # =====================
    # STEP 6: summarize
    # =====================
    candidates = collect_candidates(chroma_docs, scored)
    ranked_candidates = rerank_candidates(question, candidates, top_k=20)
    ranked_candidates = dedupe_by_similarity(ranked_candidates)

    # å‚ç…§å…ƒã®æŠ½å‡º (Webã‚½ãƒ¼ã‚¹ã®ã¿)
    sources = []
    seen_urls = set()
    for c in ranked_candidates:
        url = c["meta"].get("url")
        title = c["meta"].get("title")
        if url and url not in seen_urls:
            sources.append({"title": title, "url": url})
            seen_urls.add(url)

    # =====================
    # STEP 7: context buildï¼ˆå”¯ä¸€ï¼‰
    # =====================
    log("=== STEP 7: context build ===")

    context = build_context_from_candidates(ranked_candidates)

    log(f"[Context chars] {len(context)}")

    log("[Final Context Preview]")
    log("-----")
    log(context[:500])
    log("-----")

    # =====================
    # STEP 8: final answer
    # =====================
    answer = final_answer_pipeline(question, context, history, intent=intent, difficulty=difficulty)

    log(f"\nTotal time: {time.time() - start_time:.1f}s")
    return {"answer": answer, "sources": sources}

def analyze_document_content(text: str) -> Dict[str, Any]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’åˆ†æã—ã€è¦ç´„ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹"""
    if not text:
        return {}
    
    # å…ˆé ­3500æ–‡å­—ç¨‹åº¦ã‚’åˆ†æå¯¾è±¡ã«ã™ã‚‹
    excerpt = text[:3500]
    
    system = "You are a helpful assistant. Analyze the text and extract summary, title, and keywords."
    user = (
        f"Text:\n{excerpt}\n\n"
        "Please output the result in the following JSON format (Japanese):\n"
        "{\n"
        '  "summary": "Concise summary using bullet points",\n'
        '  "title": "A short descriptive title",\n'
        '  "keywords": ["keyword1", "keyword2", "keyword3"]\n'
        "}"
    )
    
    try:
        resp = lmstudio_chat(
            [{"role": "system", "content": system},
             {"role": "user", "content": user}],
            max_tokens=350,
            temperature=0.2
        )
        content = resp["choices"][0]["message"]["content"].strip()
        # JSONãƒ–ãƒ­ãƒƒã‚¯ã®é™¤å»
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
            
        return safe_json_load(content) or {}
    except Exception as e:
        log(f"[Analyze] Error: {e}")
        return {}

def add_document_to_kb(text: str, source: str, doc_metadata: Optional[Dict[str, Any]] = None):
    if not text:
        return

    if doc_metadata is None:
        doc_metadata = {}

    # Simple chunking
    chunk_size = 600
    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    
    ids = [f"{source}_part{i}_{int(time.time())}" for i in range(len(chunks))]
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
    title = doc_metadata.get("title") or source
    if isinstance(title, list):
        title = " ".join([str(t) for t in title])
    elif not isinstance(title, str):
        title = str(title)

    summary = doc_metadata.get("summary", "")
    if isinstance(summary, list):
        summary = "\n".join([str(s) for s in summary])
    elif not isinstance(summary, str):
        summary = str(summary)

    keywords = doc_metadata.get("keywords", [])
    if isinstance(keywords, list):
        keywords_str = ", ".join(keywords)
    else:
        keywords_str = str(keywords)

    base_meta = {
        "source": source,
        "title": title,
        "summary": summary,
        "keywords": keywords_str
    }
    metadatas: List[Any] = [base_meta.copy() for _ in chunks]
    
    # Embedding
    embeddings = embed_model.encode([f"passage: {c}" for c in chunks])
    
    collection.add(
        ids=ids,
        documents=chunks,
        embeddings=embeddings.tolist(),
        metadatas=metadatas
    )
    log(f"[DB] Added {len(chunks)} chunks from {source}")

def clear_knowledge_base():
    try:
        all_ids = collection.get()['ids']
        if all_ids:
            collection.delete(ids=all_ids)
        log("[DB] Knowledge base cleared.")
    except Exception as e:
        log(f"[DB] clear_knowledge_base error: {e}")
        raise e

def get_all_documents() -> List[Dict[str, Any]]:
    """DBå†…ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚½ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—ã—ã¦è»½é‡åŒ–
        data = collection.get(include=['metadatas'])
        metadatas = data.get('metadatas')
        if metadatas is None:
            metadatas = []
        
        # ã‚½ãƒ¼ã‚¹åã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        docs_map = {}
        for m in metadatas:
            if m and 'source' in m:
                src = m['source']
                # æ—¢ã«ç™»éŒ²æ¸ˆã¿ã§ã‚‚ã€æƒ…å ±é‡ãŒå¤šã„ï¼ˆsummaryãŒã‚ã‚‹ï¼‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦ä¿æŒã™ã‚‹
                if src not in docs_map or (m.get('summary') and not docs_map[src].get('summary')):
                    docs_map[src] = m
        
        # ãƒªã‚¹ãƒˆåŒ–
        result = []
        for src, m in docs_map.items():
            result.append({
                "source": src,
                "title": m.get("title", src),
                "summary": m.get("summary", ""),
                "keywords": m.get("keywords", "")
            })
            
        return sorted(result, key=lambda x: x['source'])
    except Exception as e:
        log("[DB] get_all_documents error:", e)
        return []

def document_exists(source: str) -> bool:
    """æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
    try:
        # limit=1 ã§å­˜åœ¨ç¢ºèª
        result = collection.get(where={"source": source}, limit=1)
        return len(result['ids']) > 0
    except Exception as e:
        log(f"[DB] document_exists error: {e}")
        return False

def delete_document_from_kb(source: str) -> bool:
    """æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤"""
    try:
        collection.delete(where={"source": source})
        log(f"[DB] Deleted document: {source}")
        return True
    except Exception as e:
        log(f"[DB] delete_document_from_kb error: {e}")
        return False

def update_document_title(source: str, new_title: str) -> bool:
    """æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°"""
    try:
        # Get all chunks for this source
        result = collection.get(where={"source": source})
        ids = result['ids']
        metadatas = result['metadatas']
        
        if not ids:
            return False
            
        if metadatas is None:
            metadatas = []

        # Update title in all metadatas
        new_metadatas = []
        for meta in metadatas:
            if meta is None:
                m: Dict[str, Any] = {}
            else:
                m = dict(meta)
            m['title'] = new_title
            new_metadatas.append(m)
            
        collection.update(ids=ids, metadatas=new_metadatas)
        log(f"[DB] Updated title for {source} to '{new_title}'")
        return True
    except Exception as e:
        log(f"[DB] update_document_title error: {e}")
        return False

def explain_term(term: str) -> str:
    """å°‚é–€ç”¨èªã®è§£èª¬ã‚’ç”Ÿæˆã™ã‚‹"""
    system = "You are a helpful teacher. Explain the technical term concisely for a student in Japanese."
    user = f"Term: {term}\n\nExplanation:"
    try:
        resp = lmstudio_chat(
            [{"role": "system", "content": system},
             {"role": "user", "content": user}],
            max_tokens=200,
            temperature=0.2,
            timeout=LM_SHORT_TIMEOUT
        )
        return resp["choices"][0]["message"]["content"].strip()
    except Exception as e:
        log(f"[Explain] Error: {e}")
        return "è§£èª¬ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

def main():
    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:]).strip()
        print(f"è³ªå•(CLI): {question}")
    else:
        question = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    if not question:
        print("è³ªå•ãŒç©ºã§ã™ã€‚")
        return

    result = process_question(question)
    print("\n=== æœ€çµ‚å›ç­” ===")
    print(result["answer"])
    if result["sources"]:
        print("\n[å‚ç…§å…ƒ]")
        for s in result["sources"]:
            print(f"- {s['title']}: {s['url']}")


if __name__ == "__main__":
    main()